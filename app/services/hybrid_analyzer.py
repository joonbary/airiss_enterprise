# app/services/hybrid_analyzer.py
"""
AIRISS v4.0 í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ê¸°
í…ìŠ¤íŠ¸ + ì •ëŸ‰ ë°ì´í„° í†µí•© ë¶„ì„ + í¸í–¥ íƒì§€
"""

import pandas as pd
from typing import Dict, Any, Optional, List
import logging
from datetime import datetime

from app.services.text_analyzer import AIRISSTextAnalyzer
from app.services.quantitative_analyzer import QuantitativeAnalyzer

logger = logging.getLogger(__name__)

class AIRISSHybridAnalyzer:
    """í…ìŠ¤íŠ¸ + ì •ëŸ‰ í†µí•© ë¶„ì„ê¸° with í¸í–¥ íƒì§€"""
    
    def __init__(self):
        self.text_analyzer = AIRISSTextAnalyzer()
        self.quantitative_analyzer = QuantitativeAnalyzer()
        
        # í¸í–¥ íƒì§€ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.bias_detector = None
        try:
            from app.services.bias_detection import BiasDetector
            self.bias_detector = BiasDetector()
            logger.info("âœ… í¸í–¥ íƒì§€ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ")
        except ImportError:
            logger.warning("âš ï¸ í¸í–¥ íƒì§€ ëª¨ë“ˆ ì—†ìŒ - ê¸°ë³¸ ë¶„ì„ë§Œ ìˆ˜í–‰")
        
        # í†µí•© ê°€ì¤‘ì¹˜
        self.hybrid_weights = {
            'text_analysis': 0.6,
            'quantitative_analysis': 0.4
        }
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥ (í¸í–¥ íƒì§€ìš©)
        self.analysis_history = []
        
        logger.info("âœ… AIRISS v4.0 í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def comprehensive_analysis(self, uid: str, opinion: str, row_data: pd.Series) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„: í…ìŠ¤íŠ¸ + ì •ëŸ‰ + í¸í–¥ ì²´í¬"""
        
        # 1. í…ìŠ¤íŠ¸ ë¶„ì„
        text_results = {}
        for dimension in self.text_analyzer.framework.keys():
            text_results[dimension] = self.text_analyzer.analyze_text(opinion, dimension)
        
        text_overall = self.text_analyzer.calculate_overall_score(
            {dim: result["score"] for dim, result in text_results.items()}
        )
        
        # 2. ì •ëŸ‰ ë¶„ì„
        quant_data = self.quantitative_analyzer.extract_quantitative_data(row_data)
        quant_results = self.quantitative_analyzer.calculate_quantitative_score(quant_data)
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        text_weight = self.hybrid_weights['text_analysis']
        quant_weight = self.hybrid_weights['quantitative_analysis']
        
        # ë°ì´í„° í’ˆì§ˆì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •
        if quant_results["data_quality"] == "ì—†ìŒ":
            text_weight = 0.8
            quant_weight = 0.2
        elif quant_results["data_quality"] == "ë‚®ìŒ":
            text_weight = 0.7
            quant_weight = 0.3
        elif quant_results["data_quality"] == "ë†’ìŒ":
            text_weight = 0.5
            quant_weight = 0.5
        
        hybrid_score = (
            text_overall["overall_score"] * text_weight + 
            quant_results["quantitative_score"] * quant_weight
        )
        
        # 4. í†µí•© ì‹ ë¢°ë„
        hybrid_confidence = (
            text_overall.get("confidence", 70) * text_weight + 
            quant_results["confidence"] * quant_weight
        )
        
        # 5. í•˜ì´ë¸Œë¦¬ë“œ ë“±ê¸‰
        hybrid_grade_info = self._calculate_hybrid_grade(hybrid_score)
        
        # 6. ì„¤ëª…ê°€ëŠ¥ì„± ì •ë³´ ì¶”ê°€
        explainability_info = self._generate_explainability(
            text_results, quant_results, text_weight, quant_weight, hybrid_score
        )
        
        # 7. ë¶„ì„ ê²°ê³¼ ì €ì¥ (í¸í–¥ íƒì§€ìš©)
        if hasattr(row_data, 'to_dict'):
            analysis_record = {
                'uid': uid,
                'hybrid_score': hybrid_score,
                'timestamp': datetime.now()
            }
            # ë³´í˜¸ ì†ì„± ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
            for attr in ['ì„±ë³„', 'ì—°ë ¹ëŒ€', 'ë¶€ì„œ', 'ì§ê¸‰']:
                if attr in row_data:
                    analysis_record[attr] = row_data[attr]
            self.analysis_history.append(analysis_record)
        
        return {
            "text_analysis": {
                "overall_score": text_overall["overall_score"],
                "grade": text_overall["grade"],
                "dimension_scores": {dim: result["score"] for dim, result in text_results.items()},
                "dimension_details": text_results
            },
            "quantitative_analysis": quant_results,
            "hybrid_analysis": {
                "overall_score": round(hybrid_score, 1),
                "grade": hybrid_grade_info["grade"],
                "grade_description": hybrid_grade_info["grade_description"],
                "percentile": hybrid_grade_info["percentile"],
                "confidence": round(hybrid_confidence, 1),
                "analysis_composition": {
                    "text_weight": round(text_weight * 100, 1),
                    "quantitative_weight": round(quant_weight * 100, 1)
                }
            },
            "explainability": explainability_info,
            "analysis_metadata": {
                "uid": uid,
                "analysis_version": "AIRISS v4.0 - Hybrid Enhanced",
                "data_sources": {
                    "text_available": bool(opinion and opinion.strip()),
                    "quantitative_available": bool(quant_data),
                    "quantitative_data_quality": quant_results["data_quality"]
                },
                "bias_detection_available": self.bias_detector is not None
            }
        }
    
    def _calculate_hybrid_grade(self, score: float) -> Dict[str, str]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¥¼ OKë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 95:
            return {
                "grade": "OKâ˜…â˜…â˜…",
                "grade_description": "ìµœìš°ìˆ˜ ë“±ê¸‰ (TOP 1%) - v4.0 í•˜ì´ë¸Œë¦¬ë“œ",
                "percentile": "ìƒìœ„ 1%"
            }
        elif score >= 90:
            return {
                "grade": "OKâ˜…â˜…",
                "grade_description": "ìš°ìˆ˜ ë“±ê¸‰ (TOP 5%) - v4.0 í•˜ì´ë¸Œë¦¬ë“œ",
                "percentile": "ìƒìœ„ 5%"
            }
        elif score >= 85:
            return {
                "grade": "OKâ˜…",
                "grade_description": "ìš°ìˆ˜+ ë“±ê¸‰ (TOP 10%) - v4.0 í•˜ì´ë¸Œë¦¬ë“œ",
                "percentile": "ìƒìœ„ 10%"
            }
        elif score >= 80:
            return {
                "grade": "OK A",
                "grade_description": "ì–‘í˜¸ ë“±ê¸‰ (TOP 20%) - v4.0 í•˜ì´ë¸Œë¦¬ë“œ",
                "percentile": "ìƒìœ„ 20%"
            }
        elif score >= 75:
            return {
                "grade": "OK B+",
                "grade_description": "ì–‘í˜¸- ë“±ê¸‰ (TOP 30%) - v4.0 í•˜ì´ë¸Œë¦¬ë“œ",
                "percentile": "ìƒìœ„ 30%"
            }
        elif score >= 70:
            return {
                "grade": "OK B",
                "grade_description": "ë³´í†µ ë“±ê¸‰ (TOP 40%) - v4.0 í•˜ì´ë¸Œë¦¬ë“œ",
                "percentile": "ìƒìœ„ 40%"
            }
        elif score >= 60:
            return {
                "grade": "OK C",
                "grade_description": "ê°œì„ í•„ìš” ë“±ê¸‰ (TOP 60%) - v4.0 í•˜ì´ë¸Œë¦¬ë“œ",
                "percentile": "ìƒìœ„ 60%"
            }
        else:
            return {
                "grade": "OK D",
                "grade_description": "ì§‘ì¤‘ê°œì„  ë“±ê¸‰ (í•˜ìœ„ 40%) - v4.0 í•˜ì´ë¸Œë¦¬ë“œ",
                "percentile": "í•˜ìœ„ 40%"
            }
    
    def _generate_explainability(self, 
                               text_results: Dict,
                               quant_results: Dict,
                               text_weight: float,
                               quant_weight: float,
                               hybrid_score: float) -> Dict[str, Any]:
        """ì ìˆ˜ ì‚°ì¶œ ê·¼ê±° ì„¤ëª… ìƒì„±"""
        
        # ì£¼ìš” ê¸ì •/ë¶€ì • ìš”ì¸ ì¶”ì¶œ
        positive_factors = []
        negative_factors = []
        
        # í…ìŠ¤íŠ¸ ë¶„ì„ ìš”ì¸
        for dimension, result in text_results.items():
            if result['score'] >= 80:
                positive_factors.append({
                    'factor': f"{dimension}",
                    'score': result['score'],
                    'impact': result['score'] * self.text_analyzer.framework[dimension]['weight'] * text_weight,
                    'evidence': result['signals']['positive_words'][:3]
                })
            elif result['score'] < 60:
                negative_factors.append({
                    'factor': f"{dimension}",
                    'score': result['score'],
                    'impact': (100 - result['score']) * self.text_analyzer.framework[dimension]['weight'] * text_weight,
                    'evidence': result['signals']['negative_words'][:3]
                })
        
        # ì •ëŸ‰ ë¶„ì„ ìš”ì¸
        if quant_results['quantitative_score'] >= 80:
            positive_factors.append({
                'factor': "ì •ëŸ‰ì  ì„±ê³¼",
                'score': quant_results['quantitative_score'],
                'impact': quant_results['quantitative_score'] * quant_weight,
                'evidence': ["KPI ë‹¬ì„±", "ì„±ê³¼ ìš°ìˆ˜"]
            })
        elif quant_results['quantitative_score'] < 60:
            negative_factors.append({
                'factor': "ì •ëŸ‰ì  ì„±ê³¼",
                'score': quant_results['quantitative_score'],
                'impact': (100 - quant_results['quantitative_score']) * quant_weight,
                'evidence': ["KPI ë¯¸ë‹¬", "ì„±ê³¼ ë¶€ì§„"]
            })
        
        # ìƒìœ„ 3ê°œ ìš”ì¸ ì •ë ¬
        positive_factors.sort(key=lambda x: x['impact'], reverse=True)
        negative_factors.sort(key=lambda x: x['impact'], reverse=True)
        
        return {
            "score_breakdown": {
                "text_contribution": round(text_results.get('ì—…ë¬´ì„±ê³¼', {}).get('score', 50) * text_weight, 1),
                "quantitative_contribution": round(quant_results['quantitative_score'] * quant_weight, 1),
                "final_score": round(hybrid_score, 1)
            },
            "key_positive_factors": positive_factors[:3],
            "key_negative_factors": negative_factors[:3],
            "improvement_suggestions": self._generate_improvement_suggestions(negative_factors),
            "confidence_explanation": self._explain_confidence(text_results, quant_results)
        }
    
    def _generate_improvement_suggestions(self, negative_factors: List[Dict]) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        for factor in negative_factors[:3]:
            if factor['factor'] == "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜":
                suggestions.append("ğŸ’¡ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤í‚¬ í–¥ìƒ êµìœ¡ ì°¸ì—¬ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            elif factor['factor'] == "ë¦¬ë”ì‹­í˜‘ì—…":
                suggestions.append("ğŸ’¡ íŒ€ì›Œí¬ ë° í˜‘ì—… ì—­ëŸ‰ ê°•í™” í”„ë¡œê·¸ë¨ ì°¸ì—¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
            elif factor['factor'] == "ì „ë¬¸ì„±í•™ìŠµ":
                suggestions.append("ğŸ’¡ ì§ë¬´ ê´€ë ¨ ì „ë¬¸ êµìœ¡ ë° ìê²©ì¦ ì·¨ë“ì„ ì¶”ì²œí•©ë‹ˆë‹¤.")
            elif factor['factor'] == "ì—…ë¬´ì„±ê³¼":
                suggestions.append("ğŸ’¡ ëª©í‘œ ì„¤ì • ë° ì‹œê°„ ê´€ë¦¬ ê¸°ë²• í•™ìŠµì´ ë„ì›€ë  ê²ƒì…ë‹ˆë‹¤.")
            elif factor['factor'] == "ì •ëŸ‰ì  ì„±ê³¼":
                suggestions.append("ğŸ’¡ KPI ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return suggestions[:3]  # ìµœëŒ€ 3ê°œ ì œì•ˆ
    
    def _explain_confidence(self, text_results: Dict, quant_results: Dict) -> str:
        """ì‹ ë¢°ë„ ì„¤ëª…"""
        avg_text_confidence = sum(r.get('confidence', 0) for r in text_results.values()) / len(text_results)
        quant_confidence = quant_results.get('confidence', 0)
        
        if avg_text_confidence >= 80 and quant_confidence >= 80:
            return "ë†’ì€ ì‹ ë¢°ë„: ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ì •ë³´ì™€ ì •ëŸ‰ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤."
        elif avg_text_confidence >= 60 or quant_confidence >= 60:
            return "ì¤‘ê°„ ì‹ ë¢°ë„: ì¼ë¶€ ë°ì´í„°ê°€ ì œí•œì ì´ì§€ë§Œ ì˜ë¯¸ìˆëŠ” ë¶„ì„ì´ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤."
        else:
            return "ë‚®ì€ ì‹ ë¢°ë„: ì œí•œëœ ì •ë³´ë¡œ ì¸í•´ ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
    
    def detect_bias_in_batch(self, analysis_results_df: pd.DataFrame) -> Dict[str, Any]:
        """ë°°ì¹˜ ë¶„ì„ ê²°ê³¼ì˜ í¸í–¥ íƒì§€"""
        if not self.bias_detector:
            return {
                "error": "í¸í–¥ íƒì§€ ì‹œìŠ¤í…œì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "recommendation": "bias_detection ëª¨ë“ˆì„ ì„¤ì¹˜í•˜ì„¸ìš”."
            }
        
        try:
            bias_report = self.bias_detector.detect_bias(analysis_results_df)
            return bias_report
        except Exception as e:
            logger.error(f"í¸í–¥ íƒì§€ ì˜¤ë¥˜: {e}")
            return {
                "error": f"í¸í–¥ íƒì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "recommendation": "ë°ì´í„° í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”."
            }
    
    def get_fairness_metrics(self) -> Dict[str, Any]:
        """í˜„ì¬ê¹Œì§€ ë¶„ì„ëœ ë°ì´í„°ì˜ ê³µì •ì„± ë©”íŠ¸ë¦­"""
        if not self.analysis_history:
            return {
                "status": "no_data",
                "message": "ì•„ì§ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            }
        
        df = pd.DataFrame(self.analysis_history)
        
        metrics = {
            "total_analyzed": len(df),
            "average_score": round(df['hybrid_score'].mean(), 1),
            "score_std": round(df['hybrid_score'].std(), 1),
            "score_distribution": {
                "min": round(df['hybrid_score'].min(), 1),
                "25%": round(df['hybrid_score'].quantile(0.25), 1),
                "50%": round(df['hybrid_score'].quantile(0.50), 1),
                "75%": round(df['hybrid_score'].quantile(0.75), 1),
                "max": round(df['hybrid_score'].max(), 1)
            }
        }
        
        # ë³´í˜¸ ì†ì„±ë³„ í‰ê·  ì ìˆ˜
        for attr in ['ì„±ë³„', 'ì—°ë ¹ëŒ€', 'ë¶€ì„œ', 'ì§ê¸‰']:
            if attr in df.columns:
                metrics[f'{attr}_averages'] = df.groupby(attr)['hybrid_score'].mean().to_dict()
        
        return metrics
