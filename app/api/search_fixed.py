# app/api/search_fixed.py - SQLite Connection ì˜¤ë¥˜ ì™„ì „ í•´ê²° ë²„ì „
# ğŸ¯ "threads can only be started once" ì˜¤ë¥˜ ì™„ì „ í•´ê²°
# AIRISS v4.1 ê³ ê¸‰ ê²€ìƒ‰ ë° ì¡°íšŒ API
# ğŸ¯ ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆ ì‚¬ìš©: files, jobs, results

from fastapi import APIRouter, HTTPException, Query, Depends
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import logging
import traceback
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from sqlalchemy import text
from collections import Counter

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ë¼ìš°í„° ìƒì„±
router = APIRouter(prefix="/search-fixed", tags=["search-fixed"])

def get_db_service():
    """DB ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° - í•­ìƒ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    from app.db.sqlite_service import SQLiteService
    return SQLiteService()

# ğŸ†• ê²€ìƒ‰ ìš”ì²­ ëª¨ë¸
class SearchRequest(BaseModel):
    query: Optional[str] = None  # í†µí•© ê²€ìƒ‰ì–´
    uid: Optional[str] = None    # íŠ¹ì • ì§ì› ID
    department: Optional[str] = None  # ë¶€ì„œ
    grade: Optional[str] = None  # ë“±ê¸‰ (OKâ˜…â˜…â˜…, OKâ˜…â˜…, etc.)
    score_min: Optional[float] = None  # ìµœì†Œ ì ìˆ˜
    score_max: Optional[float] = None  # ìµœëŒ€ ì ìˆ˜
    date_from: Optional[str] = None   # ë¶„ì„ ì‹œì‘ ë‚ ì§œ
    date_to: Optional[str] = None     # ë¶„ì„ ì¢…ë£Œ ë‚ ì§œ
    sort_by: str = "score"  # ì •ë ¬ ê¸°ì¤€: score, date, name, grade
    sort_order: str = "desc"  # ì •ë ¬ ìˆœì„œ: asc, desc
    page: int = 1           # í˜ì´ì§€ ë²ˆí˜¸
    page_size: int = 20     # í˜ì´ì§€ í¬ê¸°
    include_details: bool = False  # ìƒì„¸ ì •ë³´ í¬í•¨ ì—¬ë¶€

class AutocompleteRequest(BaseModel):
    query: str
    field: str = "uid"  # uid, department, name
    limit: int = 10

class CompareRequest(BaseModel):
    uids: List[str]  # ë¹„êµí•  ì§ì› ID ëª©ë¡
    dimensions: Optional[List[str]] = None  # ë¹„êµí•  ì°¨ì›

# ğŸ”¥ í•µì‹¬ í•´ê²°: ì»¤ë„¥ì…˜ ì¬ì‚¬ìš© í•¨ìˆ˜ ì¶”ê°€
async def execute_with_single_connection(db_service, queries_and_params):
    """
    ë‹¨ì¼ ì»¤ë„¥ì…˜ìœ¼ë¡œ ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
    
    Args:
        db_service: SQLite ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        queries_and_params: [(query1, params1), (query2, params2), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        [result1, result2, ...] í˜•íƒœì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    
    # ğŸ”¥ í•µì‹¬: í•˜ë‚˜ì˜ ì»¤ë„¥ì…˜ìœ¼ë¡œ ëª¨ë“  ì¿¼ë¦¬ ì‹¤í–‰
    async with aiosqlite.connect(db_service.db_path) as conn:
        for query, params in queries_and_params:
            cursor = await conn.execute(query, params)
            if query.strip().upper().startswith('SELECT'):
                if "COUNT(" in query.upper():
                    # COUNT ì¿¼ë¦¬ëŠ” ë‹¨ì¼ ê°’ ë°˜í™˜
                    result = await cursor.fetchone()
                    results.append(result[0] if result else 0)
                else:
                    # ì¼ë°˜ SELECT ì¿¼ë¦¬ëŠ” ëª¨ë“  í–‰ ë°˜í™˜
                    result = await cursor.fetchall()
                    results.append(result)
            else:
                # INSERT, UPDATE, DELETE ë“±
                await conn.commit()
                results.append(cursor.rowcount)
    
    return results

# ğŸ¯ í†µí•© ê²€ìƒ‰ API - ì˜¬ë°”ë¥¸ í…Œì´ë¸”ëª… ì‚¬ìš©
@router.post("/results")
async def search_analysis_results(request: SearchRequest):
    """
    ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥ - ì˜¬ë°”ë¥¸ í…Œì´ë¸”ëª… ì‚¬ìš©
    âœ… ì‹¤ì œ DB ìŠ¤í‚¤ë§ˆ: files, jobs, results
    """
    try:
        logger.info(f"ğŸ” ê²€ìƒ‰ ìš”ì²­ (ì˜¬ë°”ë¥¸ í…Œì´ë¸”): {request}")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        # âœ… ì˜¬ë°”ë¥¸ í…Œì´ë¸”ëª… ì‚¬ìš©: results, jobs
        base_query = """
        SELECT DISTINCT
            r.uid,
            r.result_data,
            j.created_at as analysis_date,
            j.file_id,
            j.id as job_id
        FROM results r
        JOIN jobs j ON r.job_id = j.id
        WHERE j.status = 'completed'
        """
        
        params = []
        conditions = []
        
        # ê²€ìƒ‰ ì¡°ê±´ ì¶”ê°€
        if request.uid:
            conditions.append("r.uid LIKE ?")
            params.append(f"%{request.uid}%")
        
        if request.query:
            conditions.append("(r.uid LIKE ? OR r.result_data LIKE ?)")
            params.extend([f"%{request.query}%", f"%{request.query}%"])
        
        if request.department:
            conditions.append("r.result_data LIKE ?")
            params.append(f'%"ë¶€ì„œ":"%{request.department}%"%')
        
        if request.grade:
            conditions.append("r.result_data LIKE ?")
            params.append(f'%"OKë“±ê¸‰":"{request.grade}"%')
        
        # ì ìˆ˜ ë²”ìœ„ í•„í„°ë§
        if request.score_min is not None:
            conditions.append("CAST(json_extract(r.result_data, '$.AIRISS_v4_ì¢…í•©ì ìˆ˜') AS REAL) >= ?")
            params.append(request.score_min)
        
        if request.score_max is not None:
            conditions.append("CAST(json_extract(r.result_data, '$.AIRISS_v4_ì¢…í•©ì ìˆ˜') AS REAL) <= ?")
            params.append(request.score_max)
        
        # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
        if request.date_from:
            conditions.append("j.created_at >= ?")
            params.append(request.date_from)
        
        if request.date_to:
            conditions.append("j.created_at <= ?")
            params.append(request.date_to)
        
        # WHERE ì ˆ ì¶”ê°€
        if conditions:
            base_query += " AND " + " AND ".join(conditions)
        
        # ì •ë ¬ ì¶”ê°€
        order_mapping = {
            "score": "CAST(json_extract(r.result_data, '$.AIRISS_v4_ì¢…í•©ì ìˆ˜') AS REAL)",
            "date": "j.created_at",
            "name": "r.uid",
            "grade": "json_extract(r.result_data, '$.OKë“±ê¸‰')"
        }
        
        order_column = order_mapping.get(request.sort_by, order_mapping["score"])
        order_direction = "DESC" if request.sort_order.lower() == "desc" else "ASC"
        base_query += f" ORDER BY {order_column} {order_direction}"
        
        # í˜ì´ì§• ì¶”ê°€
        offset = (request.page - 1) * request.page_size
        
        # ë‹¨ì¼ ì»¤ë„¥ì…˜ìœ¼ë¡œ ì¿¼ë¦¬ ì‹¤í–‰
        results = []
        total_count = 0
        
        try:
            conn = await db_service.get_connection()
            
            # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ
            count_query = base_query.split("ORDER BY")[0].replace(
                "SELECT DISTINCT r.uid, r.result_data, j.created_at as analysis_date, j.file_id, j.id as job_id",
                "SELECT COUNT(DISTINCT r.uid)"
            )
            
            count_cursor = await conn.execute(count_query, params)
            count_result = await count_cursor.fetchone()
            total_count = count_result[0] if count_result else 0
            await count_cursor.close()
            
            # ë©”ì¸ ì¿¼ë¦¬ ì‹¤í–‰ (í˜ì´ì§• ì ìš©)
            paginated_query = base_query + f" LIMIT {request.page_size} OFFSET {offset}"
            cursor = await conn.execute(paginated_query, params)
            rows = await cursor.fetchall()
            await cursor.close()
            
            await conn.close()
            
        except Exception as db_error:
            logger.error(f"âŒ DB ì¿¼ë¦¬ ì˜¤ë¥˜: {db_error}")
            if 'conn' in locals():
                await conn.close()
            raise HTTPException(status_code=500, detail=f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(db_error)}")
        
        # ê²°ê³¼ ì²˜ë¦¬
        for row in rows:
            try:
                import json
                result_data = json.loads(row[1]) if isinstance(row[1], str) else row[1]
                
                basic_info = {
                    "uid": row[0],
                    "analysis_date": row[2],
                    "file_id": row[3],
                    "job_id": row[4],
                    "score": result_data.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0),
                    "grade": result_data.get("OKë“±ê¸‰", ""),
                    "grade_description": result_data.get("ë“±ê¸‰ì„¤ëª…", ""),
                    "percentile": result_data.get("ë°±ë¶„ìœ„", ""),
                    "confidence": result_data.get("ë¶„ì„ì‹ ë¢°ë„", 0)
                }
                
                if request.include_details:
                    basic_info["full_data"] = result_data
                else:
                    basic_info.update({
                        "dimension_scores": {
                            "ì—…ë¬´ì„±ê³¼": result_data.get("ì—…ë¬´ì„±ê³¼_ì ìˆ˜", 0),
                            "KPIë‹¬ì„±": result_data.get("KPIë‹¬ì„±_ì ìˆ˜", 0),
                            "íƒœë„ë§ˆì¸ë“œ": result_data.get("íƒœë„ë§ˆì¸ë“œ_ì ìˆ˜", 0),
                            "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜": result_data.get("ì»¤ë®¤ë‹ˆì¼€ì´ì…˜_ì ìˆ˜", 0),
                            "ë¦¬ë”ì‹­í˜‘ì—…": result_data.get("ë¦¬ë”ì‹­í˜‘ì—…_ì ìˆ˜", 0),
                            "ì „ë¬¸ì„±í•™ìŠµ": result_data.get("ì „ë¬¸ì„±í•™ìŠµ_ì ìˆ˜", 0),
                            "ì°½ì˜í˜ì‹ ": result_data.get("ì°½ì˜í˜ì‹ _ì ìˆ˜", 0),
                            "ì¡°ì§ì ì‘": result_data.get("ì¡°ì§ì ì‘_ì ìˆ˜", 0)
                        },
                        "analysis_mode": result_data.get("ë¶„ì„ëª¨ë“œ", ""),
                        "analysis_system": result_data.get("ë¶„ì„ì‹œìŠ¤í…œ", "")
                    })
                
                results.append(basic_info)
                
            except Exception as e:
                logger.error(f"âš ï¸ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        # ì‘ë‹µ êµ¬ì„±
        response = {
            "results": results,
            "pagination": {
                "page": request.page,
                "page_size": request.page_size,
                "total_count": total_count,
                "total_pages": (total_count + request.page_size - 1) // request.page_size if total_count > 0 else 0
            },
            "search_info": {
                "query": request.query,
                "filters_applied": len(conditions),
                "sort_by": request.sort_by,
                "sort_order": request.sort_order
            },
            "summary": {
                "found_count": len(results),
                "avg_score": round(np.mean([r["score"] for r in results]), 1) if results else 0,
                "grade_distribution": _calculate_grade_distribution(results)
            }
        }
        
        logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ (ì˜¬ë°”ë¥¸ í…Œì´ë¸”): {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

# ğŸ¯ ìë™ì™„ì„± API - ì»¤ë„¥ì…˜ ì˜¤ë¥˜ í•´ê²°
@router.post("/autocomplete")
async def get_autocomplete_suggestions(request: AutocompleteRequest):
    """ìë™ì™„ì„± ì œì•ˆ ê¸°ëŠ¥ - ì»¤ë„¥ì…˜ ì•ˆì „ ë²„ì „"""
    try:
        logger.info(f"ğŸ”¤ ìë™ì™„ì„± ìš”ì²­: {request}")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        import aiosqlite
        
        suggestions = []
        
        if request.field == "uid":
            query = """
            SELECT DISTINCT r.uid
            FROM results r
            WHERE r.uid LIKE ?
            ORDER BY r.uid
            LIMIT ?
            """
            params = [f"%{request.query}%", request.limit]
            
        elif request.field == "grade":
            query = """
            SELECT DISTINCT json_extract(r.result_data, '$.OKë“±ê¸‰') as grade
            FROM results r
            WHERE json_extract(r.result_data, '$.OKë“±ê¸‰') LIKE ?
            ORDER BY grade
            LIMIT ?
            """
            params = [f"%{request.query}%", request.limit]
            
        elif request.field == "department":
            query = """
            SELECT DISTINCT json_extract(r.result_data, '$.ë¶€ì„œ') as dept
            FROM results r
            WHERE json_extract(r.result_data, '$.ë¶€ì„œ') LIKE ?
            AND json_extract(r.result_data, '$.ë¶€ì„œ') IS NOT NULL
            ORDER BY dept
            LIMIT ?
            """
            params = [f"%{request.query}%", request.limit]
            
        else:
            return {"suggestions": [], "message": "ì§€ì›í•˜ì§€ ì•ŠëŠ” í•„ë“œì…ë‹ˆë‹¤"}
        
        # ğŸ”¥ í•´ê²°: ë‹¨ì¼ ì»¤ë„¥ì…˜ìœ¼ë¡œ ì‹¤í–‰
        results = await execute_with_single_connection(db_service, [(query, params)])
        rows = results[0]
        
        suggestions = [row[0] for row in rows if row[0]]
        
        logger.info(f"âœ… ìë™ì™„ì„±: {len(suggestions)}ê°œ ì œì•ˆ")
        return {
            "suggestions": suggestions,
            "field": request.field,
            "query": request.query,
            "total_found": len(suggestions)
        }
        
    except Exception as e:
        logger.error(f"âŒ ìë™ì™„ì„± ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ìë™ì™„ì„± ì‹¤íŒ¨: {str(e)}")

# ğŸ¯ íŠ¹ì • ì§ì› ë¶„ì„ íˆìŠ¤í† ë¦¬ - ì»¤ë„¥ì…˜ ì˜¤ë¥˜ í•´ê²°
@router.get("/employee/{uid}")
async def get_employee_history(
    uid: str,
    limit: int = Query(10, ge=1, le=100),
    include_details: bool = Query(False)
):
    """íŠ¹ì • ì§ì›ì˜ ë¶„ì„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ - ì»¤ë„¥ì…˜ ì•ˆì „ ë²„ì „"""
    try:
        logger.info(f"ğŸ‘¤ ì§ì› íˆìŠ¤í† ë¦¬ ì¡°íšŒ: {uid}")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        import aiosqlite
        
        query = """
        SELECT 
            r.result_data,
            j.created_at,
            j.id as job_id,
            j.file_id
        FROM results r
        JOIN jobs j ON r.job_id = j.id
        WHERE r.uid = ? AND j.status = 'completed'
        ORDER BY j.created_at DESC
        LIMIT ?
        """
        
        # ğŸ”¥ í•´ê²°: ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ì‹¤í–‰
        results = await execute_with_single_connection(db_service, [(query, [uid, limit])])
        rows = results[0]
        
        if not rows:
            raise HTTPException(status_code=404, detail=f"ì§ì› {uid}ì˜ ë¶„ì„ ê¸°ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        history = []
        scores = []
        grades = []
        dates = []
        
        for row in rows:
            try:
                import json
                result_data = json.loads(row[0]) if isinstance(row[0], str) else row[0]
                
                analysis_date = row[1]
                score = result_data.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0)
                grade = result_data.get("OKë“±ê¸‰", "")
                
                entry = {
                    "analysis_date": analysis_date,
                    "job_id": row[2],
                    "file_id": row[3],
                    "score": score,
                    "grade": grade,
                    "confidence": result_data.get("ë¶„ì„ì‹ ë¢°ë„", 0)
                }
                
                if include_details:
                    entry["full_data"] = result_data
                else:
                    entry["summary"] = {
                        "top_strength": result_data.get("ì£¼ìš”ê°•ì _1ì˜ì—­", ""),
                        "improvement_area": result_data.get("ê°œì„ í•„ìš”_1ì˜ì—­", ""),
                        "ai_suggestion": result_data.get("AIê°œì„ ì œì•ˆ_1", "")
                    }
                
                history.append(entry)
                scores.append(score)
                grades.append(grade)
                dates.append(analysis_date)
                
            except Exception as e:
                logger.error(f"âš ï¸ íˆìŠ¤í† ë¦¬ í•­ëª© ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        # í†µê³„ ê³„ì‚°
        if scores:
            latest_score = scores[0]
            previous_score = scores[1] if len(scores) > 1 else scores[0]
            score_change = latest_score - previous_score
            
            trend_analysis = {
                "latest_score": latest_score,
                "previous_score": previous_score,
                "score_change": round(score_change, 1),
                "trend": "ìƒìŠ¹" if score_change > 0 else "í•˜ë½" if score_change < 0 else "ìœ ì§€",
                "highest_score": max(scores),
                "lowest_score": min(scores),
                "average_score": round(np.mean(scores), 1),
                "analysis_count": len(scores)
            }
        else:
            trend_analysis = {}
        
        response = {
            "uid": uid,
            "history": history,
            "trend_analysis": trend_analysis,
            "grade_changes": _analyze_grade_changes(grades, dates),
            "total_analyses": len(history)
        }
        
        logger.info(f"âœ… ì§ì› íˆìŠ¤í† ë¦¬: {len(history)}ê°œ ë¶„ì„ ê¸°ë¡")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì§ì› íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ğŸ¯ ë‹¤ì¤‘ ì§ì› ë¹„êµ API - ì™„ì „íˆ ì¬ì‘ì„± (í•µì‹¬ ìˆ˜ì •)
@router.post("/compare")
async def compare_employees(request: CompareRequest):
    """
    ë‹¤ì¤‘ ì§ì› ì„±ê³¼ ë¹„êµ ë¶„ì„ - ì»¤ë„¥ì…˜ ì˜¤ë¥˜ ì™„ì „ í•´ê²°
    """
    try:
        logger.info(f"ğŸ”„ ì§ì› ë¹„êµ ìš”ì²­: {request.uids}")
        
        if len(request.uids) < 2:
            raise HTTPException(status_code=400, detail="ë¹„êµë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ëª…ì˜ ì§ì›ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        if len(request.uids) > 10:
            raise HTTPException(status_code=400, detail="í•œ ë²ˆì— ìµœëŒ€ 10ëª…ê¹Œì§€ë§Œ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        import aiosqlite
        
        # ğŸ”¥ í•µì‹¬ í•´ê²°: ëª¨ë“  ì§ì›ì„ í•œ ë²ˆì˜ ì¿¼ë¦¬ë¡œ ì¡°íšŒ
        uid_placeholders = ','.join(['?' for _ in request.uids])
        batch_query = f"""
        SELECT 
            r.uid,
            r.result_data, 
            j.created_at,
            ROW_NUMBER() OVER (PARTITION BY r.uid ORDER BY j.created_at DESC) as rn
        FROM results r
        JOIN jobs j ON r.job_id = j.id
        WHERE r.uid IN ({uid_placeholders}) AND j.status = 'completed'
        ORDER BY r.uid, j.created_at DESC
        """
        
        # ğŸ”¥ ë‹¨ì¼ ì»¤ë„¥ì…˜ìœ¼ë¡œ ëª¨ë“  ë°ì´í„° ì¡°íšŒ
        results = await execute_with_single_connection(db_service, [(batch_query, request.uids)])
        rows = results[0]
        
        # ê° ì§ì›ì˜ ìµœì‹  ë¶„ì„ ê²°ê³¼ë§Œ ì¶”ì¶œ
        employee_data_map = {}
        for row in rows:
            uid = row[0]
            rn = row[3]  # ROW_NUMBER
            
            # ê° ì§ì›ì˜ ì²« ë²ˆì§¸ (ìµœì‹ ) ê²°ê³¼ë§Œ ì‚¬ìš©
            if rn == 1 and uid not in employee_data_map:
                employee_data_map[uid] = row
        
        # ë°ì´í„° ë³€í™˜
        comparison_data = []
        for uid in request.uids:
            if uid not in employee_data_map:
                logger.warning(f"âš ï¸ ì§ì› {uid}ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                continue
            
            row = employee_data_map[uid]
            
            try:
                import json
                result_data = json.loads(row[1]) if isinstance(row[1], str) else row[1]
                
                employee_data = {
                    "uid": uid,
                    "analysis_date": row[2],
                    "overall_score": result_data.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0),
                    "grade": result_data.get("OKë“±ê¸‰", ""),
                    "dimension_scores": {
                        "ì—…ë¬´ì„±ê³¼": result_data.get("ì—…ë¬´ì„±ê³¼_ì ìˆ˜", 0),
                        "KPIë‹¬ì„±": result_data.get("KPIë‹¬ì„±_ì ìˆ˜", 0),
                        "íƒœë„ë§ˆì¸ë“œ": result_data.get("íƒœë„ë§ˆì¸ë“œ_ì ìˆ˜", 0),
                        "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜": result_data.get("ì»¤ë®¤ë‹ˆì¼€ì´ì…˜_ì ìˆ˜", 0),
                        "ë¦¬ë”ì‹­í˜‘ì—…": result_data.get("ë¦¬ë”ì‹­í˜‘ì—…_ì ìˆ˜", 0),
                        "ì „ë¬¸ì„±í•™ìŠµ": result_data.get("ì „ë¬¸ì„±í•™ìŠµ_ì ìˆ˜", 0),
                        "ì°½ì˜í˜ì‹ ": result_data.get("ì°½ì˜í˜ì‹ _ì ìˆ˜", 0),
                        "ì¡°ì§ì ì‘": result_data.get("ì¡°ì§ì ì‘_ì ìˆ˜", 0)
                    },
                    "strengths": [
                        result_data.get("ì£¼ìš”ê°•ì _1ì˜ì—­", ""),
                        result_data.get("ì£¼ìš”ê°•ì _2ì˜ì—­", ""),
                        result_data.get("ì£¼ìš”ê°•ì _3ì˜ì—­", "")
                    ],
                    "improvements": [
                        result_data.get("ê°œì„ í•„ìš”_1ì˜ì—­", ""),
                        result_data.get("ê°œì„ í•„ìš”_2ì˜ì—­", ""),
                        result_data.get("ê°œì„ í•„ìš”_3ì˜ì—­", "")
                    ]
                }
                
                comparison_data.append(employee_data)
                
            except Exception as e:
                logger.error(f"âš ï¸ ì§ì› {uid} ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        if len(comparison_data) < 2:
            raise HTTPException(status_code=404, detail="ë¹„êµí•  ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ë¶„ì„ ê²°ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
        
        # ë¹„êµ ë¶„ì„ ìˆ˜í–‰
        comparison_analysis = _perform_comparison_analysis(comparison_data, request.dimensions)
        
        response = {
            "employees": comparison_data,
            "comparison_analysis": comparison_analysis,
            "metadata": {
                "compared_count": len(comparison_data),
                "requested_uids": request.uids,
                "comparison_date": datetime.now().isoformat(),
                "dimensions_analyzed": request.dimensions or ["ì „ì²´"]
            }
        }
        
        logger.info(f"âœ… ì§ì› ë¹„êµ ì™„ë£Œ: {len(comparison_data)}ëª…")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì§ì› ë¹„êµ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

# ğŸ¯ íŒ€ë³„ ë¶„ì„ í˜„í™© - ì»¤ë„¥ì…˜ ì˜¤ë¥˜ í•´ê²°
@router.get("/team-summary")
async def get_team_summary(
    department: Optional[str] = Query(None),
    date_from: Optional[str] = Query(None),
    date_to: Optional[str] = Query(None)
):
    """íŒ€/ë¶€ì„œë³„ ë¶„ì„ í˜„í™© ìš”ì•½ - ì»¤ë„¥ì…˜ ì•ˆì „ ë²„ì „"""
    try:
        logger.info(f"ğŸ¢ íŒ€ ìš”ì•½ ì¡°íšŒ: ë¶€ì„œ={department}")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        import aiosqlite
        
        # ê¸°ë³¸ ì¿¼ë¦¬
        base_query = """
        SELECT 
            r.result_data,
            j.created_at
        FROM results r
        JOIN jobs j ON r.job_id = j.id
        WHERE j.status = 'completed'
        """
        
        params = []
        
        # ë¶€ì„œ í•„í„°
        if department:
            base_query += " AND r.result_data LIKE ?"
            params.append(f'%"ë¶€ì„œ":"%{department}%"%')
        
        # ë‚ ì§œ í•„í„°
        if date_from:
            base_query += " AND j.created_at >= ?"
            params.append(date_from)
        
        if date_to:
            base_query += " AND j.created_at <= ?"
            params.append(date_to)
        
        # ğŸ”¥ í•´ê²°: ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ì‹¤í–‰
        results = await execute_with_single_connection(db_service, [(base_query, params)])
        rows = results[0]
        
        # ë°ì´í„° ì²˜ë¦¬
        team_data = {}
        total_analyses = 0
        
        for row in rows:
            try:
                import json
                result_data = json.loads(row[0]) if isinstance(row[0], str) else row[0]
                
                dept = result_data.get("ë¶€ì„œ", "ë¯¸ë¶„ë¥˜")
                score = result_data.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0)
                grade = result_data.get("OKë“±ê¸‰", "")
                
                if dept not in team_data:
                    team_data[dept] = {
                        "department": dept,
                        "scores": [],
                        "grades": [],
                        "analysis_count": 0
                    }
                
                team_data[dept]["scores"].append(score)
                team_data[dept]["grades"].append(grade)
                team_data[dept]["analysis_count"] += 1
                total_analyses += 1
                
            except Exception as e:
                logger.error(f"âš ï¸ íŒ€ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        # íŒ€ë³„ í†µê³„ ê³„ì‚°
        team_summary = []
        for dept, data in team_data.items():
            if data["scores"]:
                summary = {
                    "department": dept,
                    "analysis_count": data["analysis_count"],
                    "average_score": round(np.mean(data["scores"]), 1),
                    "highest_score": max(data["scores"]),
                    "lowest_score": min(data["scores"]),
                    "grade_distribution": dict(Counter(data["grades"])),
                    "performance_level": _classify_team_performance(np.mean(data["scores"]))
                }
                team_summary.append(summary)
        
        # ì •ë ¬ (í‰ê·  ì ìˆ˜ ê¸°ì¤€)
        team_summary.sort(key=lambda x: x["average_score"], reverse=True)
        
        response = {
            "team_summary": team_summary,
            "overall_statistics": {
                "total_departments": len(team_summary),
                "total_analyses": total_analyses,
                "overall_average": round(np.mean([t["average_score"] for t in team_summary]), 1) if team_summary else 0,
                "best_performing_team": team_summary[0]["department"] if team_summary else None,
                "analysis_period": {
                    "from": date_from,
                    "to": date_to
                }
            }
        }
        
        logger.info(f"âœ… íŒ€ ìš”ì•½ ì™„ë£Œ: {len(team_summary)}ê°œ ë¶€ì„œ")
        return response
        
    except Exception as e:
        logger.error(f"âŒ íŒ€ ìš”ì•½ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"íŒ€ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ğŸ¯ ì¦ê²¨ì°¾ê¸° ê´€ë ¨ í•¨ìˆ˜ë“¤ - ì»¤ë„¥ì…˜ ì•ˆì „ ë²„ì „
class FavoriteRequest(BaseModel):
    uid: str
    user_id: str = "default_user"
    note: Optional[str] = None

# ì¦ê²¨ì°¾ê¸° ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” DB ì‚¬ìš©)
favorites_storage = {}

@router.post("/favorites/add")
async def add_favorite(request: FavoriteRequest):
    """ì¦ê²¨ì°¾ê¸° ì¶”ê°€ - ì•ˆì „ ë²„ì „"""
    try:
        logger.info(f"â­ ì¦ê²¨ì°¾ê¸° ì¶”ê°€: {request.uid}")
        
        user_favorites = favorites_storage.get(request.user_id, [])
        
        # ì¤‘ë³µ ì²´í¬
        existing = next((f for f in user_favorites if f["uid"] == request.uid), None)
        if existing:
            return {"status": "already_exists", "message": "ì´ë¯¸ ì¦ê²¨ì°¾ê¸°ì— ìˆìŠµë‹ˆë‹¤"}
        
        # ì¦ê²¨ì°¾ê¸° ì¶”ê°€
        favorite_entry = {
            "uid": request.uid,
            "note": request.note,
            "added_at": datetime.now().isoformat(),
            "id": len(user_favorites) + 1
        }
        
        user_favorites.append(favorite_entry)
        favorites_storage[request.user_id] = user_favorites
        
        logger.info(f"âœ… ì¦ê²¨ì°¾ê¸° ì¶”ê°€ ì™„ë£Œ: {request.uid}")
        return {
            "status": "added",
            "favorite": favorite_entry,
            "total_favorites": len(user_favorites)
        }
        
    except Exception as e:
        logger.error(f"âŒ ì¦ê²¨ì°¾ê¸° ì¶”ê°€ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¦ê²¨ì°¾ê¸° ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")

@router.get("/favorites")
async def get_favorites(
    user_id: str = Query("default_user"),
    include_details: bool = Query(False)
):
    """ì¦ê²¨ì°¾ê¸° ëª©ë¡ ì¡°íšŒ - ì»¤ë„¥ì…˜ ì•ˆì „ ë²„ì „"""
    try:
        logger.info(f"â­ ì¦ê²¨ì°¾ê¸° ëª©ë¡ ì¡°íšŒ: {user_id}")
        
        user_favorites = favorites_storage.get(user_id, [])
        
        if not user_favorites:
            return {
                "favorites": [],
                "total_count": 0,
                "message": "ì¦ê²¨ì°¾ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤"
            }
        
        # ìƒì„¸ ì •ë³´ í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
        if include_details:
            db_service = get_db_service()
            await db_service.init_database()
            
            import aiosqlite
            
            # ğŸ”¥ í•´ê²°: ëª¨ë“  ì¦ê²¨ì°¾ê¸° UIDë¥¼ í•œ ë²ˆì˜ ì¿¼ë¦¬ë¡œ ì¡°íšŒ
            favorite_uids = [f["uid"] for f in user_favorites]
            uid_placeholders = ','.join(['?' for _ in favorite_uids])
            
            batch_query = f"""
            SELECT 
                r.uid,
                r.result_data, 
                j.created_at,
                ROW_NUMBER() OVER (PARTITION BY r.uid ORDER BY j.created_at DESC) as rn
            FROM results r
            JOIN jobs j ON r.job_id = j.id
            WHERE r.uid IN ({uid_placeholders}) AND j.status = 'completed'
            ORDER BY r.uid, j.created_at DESC
            """
            
            results = await execute_with_single_connection(db_service, [(batch_query, favorite_uids)])
            rows = results[0]
            
            # ìµœì‹  ë¶„ì„ ê²°ê³¼ ë§¤í•‘
            latest_analysis = {}
            for row in rows:
                uid = row[0]
                rn = row[3]
                if rn == 1:  # ìµœì‹  ê²°ê³¼ë§Œ
                    latest_analysis[uid] = {
                        "result_data": row[1],
                        "created_at": row[2]
                    }
            
            # ì¦ê²¨ì°¾ê¸°ì— ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            detailed_favorites = []
            for favorite in user_favorites:
                uid = favorite["uid"]
                detailed_favorite = favorite.copy()
                
                if uid in latest_analysis:
                    try:
                        import json
                        result_data = json.loads(latest_analysis[uid]["result_data"]) if isinstance(latest_analysis[uid]["result_data"], str) else latest_analysis[uid]["result_data"]
                        
                        detailed_favorite.update({
                            "latest_score": result_data.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0),
                            "latest_grade": result_data.get("OKë“±ê¸‰", ""),
                            "last_analysis": latest_analysis[uid]["created_at"],
                            "has_analysis": True
                        })
                    except Exception as e:
                        logger.error(f"âš ï¸ ì¦ê²¨ì°¾ê¸° ë¶„ì„ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜ ({uid}): {e}")
                        detailed_favorite.update({"has_analysis": False, "error": str(e)})
                else:
                    detailed_favorite.update({
                        "has_analysis": False,
                        "message": "ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
                    })
                
                detailed_favorites.append(detailed_favorite)
            
            return {
                "favorites": detailed_favorites,
                "total_count": len(detailed_favorites),
                "include_details": True
            }
        else:
            return {
                "favorites": user_favorites,
                "total_count": len(user_favorites),
                "include_details": False
            }
        
    except Exception as e:
        logger.error(f"âŒ ì¦ê²¨ì°¾ê¸° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¦ê²¨ì°¾ê¸° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# ë‚˜ë¨¸ì§€ í•¨ìˆ˜ë“¤ (ë³€ê²½ ì—†ìŒ)
def _calculate_grade_distribution(results):
    """ë“±ê¸‰ë³„ ë¶„í¬ ê³„ì‚°"""
    if not results:
        return {}
    
    grade_counts = {}
    for result in results:
        grade = result.get("grade", "Unknown")
        grade_counts[grade] = grade_counts.get(grade, 0) + 1
    
    return grade_counts

def _analyze_grade_changes(grades, dates):
    """ë“±ê¸‰ ë³€í™” ë¶„ì„"""
    if len(grades) < 2:
        return {"message": "ë“±ê¸‰ ë³€í™”ë¥¼ ë¶„ì„í•˜ê¸°ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
    
    latest_grade = grades[0]
    previous_grade = grades[1]
    
    # ë“±ê¸‰ ì ìˆ˜ ë§¤í•‘
    grade_scores = {
        "OKâ˜…â˜…â˜…": 100, "OKâ˜…â˜…": 90, "OKâ˜…": 85, "OK A": 80,
        "OK B+": 75, "OK B": 70, "OK C": 60, "OK D": 40
    }
    
    latest_score = grade_scores.get(latest_grade, 50)
    previous_score = grade_scores.get(previous_grade, 50)
    
    return {
        "latest_grade": latest_grade,
        "previous_grade": previous_grade,
        "grade_change": "ìƒìŠ¹" if latest_score > previous_score else "í•˜ë½" if latest_score < previous_score else "ìœ ì§€",
        "grade_history": grades[:5]  # ìµœê·¼ 5ê°œ
    }

def _perform_comparison_analysis(employees_data, dimensions=None):
    """ë¹„êµ ë¶„ì„ ìˆ˜í–‰"""
    if not employees_data:
        return {}
    
    # ì¢…í•© ì ìˆ˜ ë¹„êµ
    scores = [emp["overall_score"] for emp in employees_data]
    highest_performer = max(employees_data, key=lambda x: x["overall_score"])
    lowest_performer = min(employees_data, key=lambda x: x["overall_score"])
    
    # ì°¨ì›ë³„ ë¹„êµ
    dimension_comparison = {}
    all_dimensions = ["ì—…ë¬´ì„±ê³¼", "KPIë‹¬ì„±", "íƒœë„ë§ˆì¸ë“œ", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", 
                     "ë¦¬ë”ì‹­í˜‘ì—…", "ì „ë¬¸ì„±í•™ìŠµ", "ì°½ì˜í˜ì‹ ", "ì¡°ì§ì ì‘"]
    
    target_dimensions = dimensions if dimensions else all_dimensions
    
    for dimension in target_dimensions:
        if dimension in all_dimensions:
            dim_scores = [emp["dimension_scores"].get(dimension, 0) for emp in employees_data]
            dimension_comparison[dimension] = {
                "scores": dict(zip([emp["uid"] for emp in employees_data], dim_scores)),
                "highest": max(dim_scores),
                "lowest": min(dim_scores),
                "average": round(np.mean(dim_scores), 1),
                "range": max(dim_scores) - min(dim_scores)
            }
    
    return {
        "overall_comparison": {
            "highest_performer": {
                "uid": highest_performer["uid"],
                "score": highest_performer["overall_score"],
                "grade": highest_performer["grade"]
            },
            "lowest_performer": {
                "uid": lowest_performer["uid"],
                "score": lowest_performer["overall_score"],
                "grade": lowest_performer["grade"]
            },
            "score_range": max(scores) - min(scores),
            "average_score": round(np.mean(scores), 1)
        },
        "dimension_comparison": dimension_comparison,
        "insights": _generate_comparison_insights(employees_data)
    }

def _generate_comparison_insights(employees_data):
    """ë¹„êµ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    insights = []
    
    if len(employees_data) >= 2:
        scores = [emp["overall_score"] for emp in employees_data]
        score_range = max(scores) - min(scores)
        
        if score_range > 20:
            insights.append("ì§ì› ê°„ ì„±ê³¼ í¸ì°¨ê°€ í½ë‹ˆë‹¤. ì €ì„±ê³¼ìì— ëŒ€í•œ ì§‘ì¤‘ ì§€ì›ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif score_range < 5:
            insights.append("ì§ì› ê°„ ì„±ê³¼ê°€ ê· ë“±í•©ë‹ˆë‹¤. íŒ€ ì „ì²´ì˜ ì•ˆì •ì ì¸ ì„±ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        
        # ê°•ì  ë¶„ì„
        all_strengths = []
        for emp in employees_data:
            all_strengths.extend([s for s in emp["strengths"] if s])
        
        common_strengths = Counter(all_strengths).most_common(3)
        
        if common_strengths:
            insights.append(f"ê³µí†µ ê°•ì  ì˜ì—­: {', '.join([s[0] for s in common_strengths])}")
    
    return insights

def _classify_team_performance(avg_score):
    """íŒ€ ì„±ê³¼ ìˆ˜ì¤€ ë¶„ë¥˜"""
    if avg_score >= 90:
        return "ìµœìš°ìˆ˜"
    elif avg_score >= 80:
        return "ìš°ìˆ˜"
    elif avg_score >= 70:
        return "ì–‘í˜¸"
    elif avg_score >= 60:
        return "ë³´í†µ"
    else:
        return "ê°œì„ í•„ìš”"

# ê¸°íƒ€ ì—”ë“œí¬ì¸íŠ¸ë“¤ (ê°„ì†Œí™”)
@router.delete("/favorites/remove/{uid}")
async def remove_favorite(uid: str, user_id: str = Query("default_user")):
    """ì¦ê²¨ì°¾ê¸° ì œê±°"""
    try:
        user_favorites = favorites_storage.get(user_id, [])
        original_count = len(user_favorites)
        user_favorites = [f for f in user_favorites if f["uid"] != uid]
        
        if len(user_favorites) == original_count:
            raise HTTPException(status_code=404, detail="ì¦ê²¨ì°¾ê¸°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        favorites_storage[user_id] = user_favorites
        return {"status": "removed", "uid": uid, "remaining_count": len(user_favorites)}
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì¦ê²¨ì°¾ê¸° ì œê±° ì‹¤íŒ¨: {str(e)}")

@router.get("/health")
async def search_health_check():
    """ê²€ìƒ‰ API í—¬ìŠ¤ì²´í¬ - ì˜¬ë°”ë¥¸ í…Œì´ë¸”ëª… ì‚¬ìš©"""
    try:
        db_service = get_db_service()
        
        # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        try:
            conn = await db_service.get_connection()
            
            # í…Œì´ë¸” ëª©ë¡ í™•ì¸
            cursor = await conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = await cursor.fetchall()
            table_names = [table[0] for table in tables]
            await cursor.close()
            
            # ê° í…Œì´ë¸”ì˜ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
            table_counts = {}
            for table_name in ['files', 'jobs', 'results']:
                if table_name in table_names:
                    cursor = await conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = (await cursor.fetchone())[0]
                    table_counts[table_name] = count
                    await cursor.close()
                else:
                    table_counts[table_name] = "ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
            
            await conn.close()
            
        except Exception as db_error:
            if 'conn' in locals():
                await conn.close()
            table_counts = {"error": str(db_error)}
        
        return {
            "status": "healthy",
            "service": "AIRISS Search API v4.1 - ì˜¬ë°”ë¥¸ í…Œì´ë¸”ëª… ì‚¬ìš©",
            "database_tables": table_counts,
            "correct_tables": ["files", "jobs", "results"],
            "api_endpoint": "/search-fixed/results",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ğŸ¯ í…Œì´ë¸” ì •ë³´ í™•ì¸
@router.get("/debug/tables")
async def debug_tables():
    """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì •ë³´ í™•ì¸"""
    try:
        db_service = get_db_service()
        conn = await db_service.get_connection()
        
        # ëª¨ë“  í…Œì´ë¸” ëª©ë¡
        cursor = await conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = await cursor.fetchall()
        await cursor.close()
        
        table_info = {}
        for table in tables:
            table_name = table[0]
            
            # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ
            cursor = await conn.execute(f"PRAGMA table_info({table_name})")
            schema = await cursor.fetchall()
            await cursor.close()
            
            # ë ˆì½”ë“œ ìˆ˜
            cursor = await conn.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = (await cursor.fetchone())[0]
            await cursor.close()
            
            table_info[table_name] = {
                "columns": [{"name": col[1], "type": col[2]} for col in schema],
                "record_count": count
            }
        
        await conn.close()
        
        return {
            "database_tables": table_info,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        if 'conn' in locals():
            await conn.close()
        return {"error": str(e)}
