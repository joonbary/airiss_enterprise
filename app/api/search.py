# app/api/search_fixed_v2.py
# AIRISS v4.1 ê³ ê¸‰ ê²€ìƒ‰ ë° ì¡°íšŒ API - SQLite ì»¤ë„¥ì…˜ ë¬¸ì œ í•´ê²°
# ğŸ¯ ë¶„ì„ê²°ê³¼ ì¡°íšŒ ê¸°ëŠ¥ ê°•í™” - ì»¤ë„¥ì…˜ ìµœì í™”

from fastapi import APIRouter, HTTPException, Query, Depends
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import logging
import traceback
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from sqlalchemy import text
import asyncio
from collections import Counter
import re

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ë¼ìš°í„° ìƒì„±
router = APIRouter(prefix="/search", tags=["search"])

def get_db_service():
    """DB ì„œë¹„ìŠ¤ ê°€ì ¸ì˜¤ê¸° - í•­ìƒ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    from app.db.sqlite_service import SQLiteService
    return SQLiteService()

# ğŸ†• ê²€ìƒ‰ ìš”ì²­ ëª¨ë¸
class SearchRequest(BaseModel):
    query: Optional[str] = None  # í†µí•© ê²€ìƒ‰ì–´
    uid: Optional[str] = None    # íŠ¹ì • ì§ì› ID
    department: Optional[str] = None  # ë¶€ì„œ
    grade: Optional[str] = None  # ë“±ê¸‰ (OKâ˜…â˜…â˜…, OKâ˜…â˜…, etc.)
    score_min: Optional[float] = None  # ìµœì†Œ ì ìˆ˜
    score_max: Optional[float] = None  # ìµœëŒ€ ì ìˆ˜
    date_from: Optional[str] = None   # ë¶„ì„ ì‹œì‘ ë‚ ì§œ
    date_to: Optional[str] = None     # ë¶„ì„ ì¢…ë£Œ ë‚ ì§œ
    sort_by: str = "score"  # ì •ë ¬ ê¸°ì¤€: score, date, name, grade
    sort_order: str = "desc"  # ì •ë ¬ ìˆœì„œ: asc, desc
    page: int = 1           # í˜ì´ì§€ ë²ˆí˜¸
    page_size: int = 20     # í˜ì´ì§€ í¬ê¸°
    include_details: bool = False  # ìƒì„¸ ì •ë³´ í¬í•¨ ì—¬ë¶€

class AutocompleteRequest(BaseModel):
    query: str
    field: str = "uid"  # uid, department, name
    limit: int = 10

class CompareRequest(BaseModel):
    uids: List[str]  # ë¹„êµí•  ì§ì› ID ëª©ë¡
    dimensions: Optional[List[str]] = None  # ë¹„êµí•  ì°¨ì›

class FavoriteRequest(BaseModel):
    uid: str
    user_id: str = "default_user"  # ì‹¤ì œë¡œëŠ” ì¸ì¦ì—ì„œ ê°€ì ¸ì˜´
    note: Optional[str] = None  # ì¦ê²¨ì°¾ê¸° ë©”ëª¨

# ğŸ¯ í†µí•© ê²€ìƒ‰ API
@router.post("/results")
async def search_analysis_results(request: SearchRequest):
    """
    ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥ - ë‹¤ì–‘í•œ ì¡°ê±´ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ ê²€ìƒ‰
    âœ… ë‹¨ì¼ ì»¤ë„¥ì…˜ìœ¼ë¡œ ìµœì í™”
    """
    try:
        logger.info(f"ğŸ” ê³ ê¸‰ ê²€ìƒ‰ ìš”ì²­: {request}")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        # SQL ì¿¼ë¦¬ ë¹Œë“œ
        base_query = """
        SELECT DISTINCT
            r.uid,
            r.result_data,
            j.created_at as analysis_date,
            j.file_id,
            j.id as job_id
        FROM results r
        JOIN jobs j ON r.job_id = j.id
        WHERE j.status = 'completed'
        """
        
        params = []
        conditions = []
        
        # ğŸ” ê²€ìƒ‰ ì¡°ê±´ ì¶”ê°€ (ì¿¼ë¦¬-íŒŒë¼ë¯¸í„° ê°œìˆ˜ ì¼ì¹˜ ë³´ì¥)
        if request.uid and request.query:
            conditions.append("(r.uid LIKE ? OR r.result_data LIKE ?)")
            params.extend([f"%{request.uid}%", f"%{request.query}%"])
        elif request.uid:
            conditions.append("r.uid LIKE ?")
            params.append(f"%{request.uid}%")
        elif request.query:
            conditions.append("r.result_data LIKE ?")
            params.append(f"%{request.query}%")
        
        if request.department:
            conditions.append("r.result_data LIKE ?")
            params.append(f'%"ë¶€ì„œ":"%{request.department}%"%')
        
        if request.grade:
            conditions.append("r.result_data LIKE ?")
            params.append(f'%"OKë“±ê¸‰":"{request.grade}"%')
        
        # ì ìˆ˜ ë²”ìœ„ í•„í„°ë§
        if request.score_min is not None:
            conditions.append("CAST(json_extract(r.result_data, '$.AIRISS_v4_ì¢…í•©ì ìˆ˜') AS REAL) >= ?")
            params.append(request.score_min)
        
        if request.score_max is not None:
            conditions.append("CAST(json_extract(r.result_data, '$.AIRISS_v4_ì¢…í•©ì ìˆ˜') AS REAL) <= ?")
            params.append(request.score_max)
        
        # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
        if request.date_from:
            conditions.append("j.created_at >= ?")
            params.append(request.date_from)
        
        if request.date_to:
            conditions.append("j.created_at <= ?")
            params.append(request.date_to)
        
        # WHERE ì ˆ ì¶”ê°€
        if conditions:
            base_query += " AND " + " AND ".join(conditions)
        
        # ì •ë ¬ ì¶”ê°€
        order_mapping = {
            "score": "CAST(json_extract(r.result_data, '$.AIRISS_v4_ì¢…í•©ì ìˆ˜') AS REAL)",
            "date": "j.created_at",
            "name": "r.uid",
            "grade": "json_extract(r.result_data, '$.OKë“±ê¸‰')"
        }
        
        order_column = order_mapping.get(request.sort_by, order_mapping["score"])
        order_direction = "DESC" if request.sort_order.lower() == "desc" else "ASC"
        base_query += f" ORDER BY {order_column} {order_direction}"
        
        # í˜ì´ì§• ì¶”ê°€
        offset = (request.page - 1) * request.page_size
        
        # ğŸ”¥ ìˆ˜ì •: ë‹¨ì¼ ì»¤ë„¥ì…˜ìœ¼ë¡œ ëª¨ë“  ì¿¼ë¦¬ ì‹¤í–‰
        results = []
        total_count = 0
        
        try:
            # ë‹¨ì¼ ì»¤ë„¥ì…˜ ìƒì„±
            conn = await db_service.get_connection()
            
            # ì „ì²´ ê°œìˆ˜ ì¡°íšŒ (ë¨¼ì €)
            count_query = re.sub(
                r"SELECT DISTINCT[\s\S]+?FROM",
                "SELECT COUNT(DISTINCT r.uid) FROM",
                base_query.split("ORDER BY")[0],
                flags=re.MULTILINE
            )
            count_params = params[:len(conditions)]
            logger.debug(f"[COUNT-QUERY] SQL: {count_query}")
            logger.debug(f"[COUNT-QUERY] PARAMS: {count_params}")
            count_cursor = await conn.execute(count_query, count_params)
            count_result = await count_cursor.fetchone()
            total_count = int(count_result[0]) if count_result else 0
            await count_cursor.close()
            
            # ë©”ì¸ ì¿¼ë¦¬ ì‹¤í–‰ (í˜ì´ì§• ì ìš©)
            paginated_query = base_query + f" LIMIT {request.page_size} OFFSET {offset}"
            cursor = await conn.execute(paginated_query, params)
            rows = await cursor.fetchall()
            await cursor.close()
            
            # ì»¤ë„¥ì…˜ ì¢…ë£Œ
            await conn.close()
            
        except Exception as db_error:
            logger.error(f"âŒ DB ì¿¼ë¦¬ ì˜¤ë¥˜: {db_error}")
            if 'conn' in locals():
                await conn.close()
            raise HTTPException(status_code=500, detail=f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(db_error)}")
        
        # ê²°ê³¼ ì²˜ë¦¬
        for row in rows:
            try:
                import json
                result_data = json.loads(row[1]) if isinstance(row[1], str) else row[1]
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                basic_info = {
                    "uid": row[0],
                    "analysis_date": row[2],
                    "file_id": row[3],
                    "job_id": row[4],
                    "score": result_data.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0),
                    "grade": result_data.get("OKë“±ê¸‰", ""),
                    "grade_description": result_data.get("ë“±ê¸‰ì„¤ëª…", ""),
                    "percentile": result_data.get("ë°±ë¶„ìœ„", ""),
                    "confidence": result_data.get("ë¶„ì„ì‹ ë¢°ë„", 0)
                }
                
                # ìƒì„¸ ì •ë³´ í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ ë°ì´í„° ì¶”ê°€
                if request.include_details:
                    basic_info["full_data"] = result_data
                else:
                    # í•µì‹¬ ì •ë³´ë§Œ ì¶”ê°€
                    basic_info.update({
                        "dimension_scores": {
                            "ì—…ë¬´ì„±ê³¼": result_data.get("ì—…ë¬´ì„±ê³¼_ì ìˆ˜", 0),
                            "KPIë‹¬ì„±": result_data.get("KPIë‹¬ì„±_ì ìˆ˜", 0),
                            "íƒœë„ë§ˆì¸ë“œ": result_data.get("íƒœë„ë§ˆì¸ë“œ_ì ìˆ˜", 0),
                            "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜": result_data.get("ì»¤ë®¤ë‹ˆì¼€ì´ì…˜_ì ìˆ˜", 0),
                            "ë¦¬ë”ì‹­í˜‘ì—…": result_data.get("ë¦¬ë”ì‹­í˜‘ì—…_ì ìˆ˜", 0),
                            "ì „ë¬¸ì„±í•™ìŠµ": result_data.get("ì „ë¬¸ì„±í•™ìŠµ_ì ìˆ˜", 0),
                            "ì°½ì˜í˜ì‹ ": result_data.get("ì°½ì˜í˜ì‹ _ì ìˆ˜", 0),
                            "ì¡°ì§ì ì‘": result_data.get("ì¡°ì§ì ì‘_ì ìˆ˜", 0)
                        },
                        "analysis_mode": result_data.get("ë¶„ì„ëª¨ë“œ", ""),
                        "analysis_system": result_data.get("ë¶„ì„ì‹œìŠ¤í…œ", "")
                    })
                
                results.append(basic_info)
                
            except Exception as e:
                logger.error(f"âš ï¸ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        # ì‘ë‹µ êµ¬ì„±
        response = {
            "results": results,
            "pagination": {
                "page": request.page,
                "page_size": request.page_size,
                "total_count": total_count,
                "total_pages": (total_count + request.page_size - 1) // request.page_size if total_count > 0 else 0
            },
            "search_info": {
                "query": request.query,
                "filters_applied": len(conditions),
                "sort_by": request.sort_by,
                "sort_order": request.sort_order
            },
            "summary": {
                "found_count": len(results),
                "avg_score": round(np.mean([r["score"] for r in results]), 1) if results else 0,
                "grade_distribution": _calculate_grade_distribution(results)
            }
        }
        
        logger.info(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼ ë°˜í™˜ (ì „ì²´ {total_count}ê°œ)")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

def _calculate_grade_distribution(results):
    """ë“±ê¸‰ë³„ ë¶„í¬ ê³„ì‚°"""
    if not results:
        return {}
    
    grade_counts = {}
    for result in results:
        grade = result.get("grade", "Unknown")
        grade_counts[grade] = grade_counts.get(grade, 0) + 1
    
    return grade_counts

# ğŸ¯ ìë™ì™„ì„± API - ìµœì í™”
@router.post("/autocomplete")
async def get_autocomplete_suggestions(request: AutocompleteRequest):
    """ìë™ì™„ì„± ì œì•ˆ ê¸°ëŠ¥ - ë‹¨ì¼ ì»¤ë„¥ì…˜ ì‚¬ìš©"""
    try:
        logger.info(f"ğŸ”¤ ìë™ì™„ì„± ìš”ì²­: {request}")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        suggestions = []
        query = ""
        params = []
        
        if request.field == "uid":
            query = """
            SELECT DISTINCT r.uid
            FROM results r
            WHERE r.uid LIKE ?
            ORDER BY r.uid
            LIMIT ?
            """
            params = [f"%{request.query}%", request.limit]
            
        elif request.field == "grade":
            query = """
            SELECT DISTINCT json_extract(r.result_data, '$.OKë“±ê¸‰') as grade
            FROM results r
            WHERE json_extract(r.result_data, '$.OKë“±ê¸‰') LIKE ?
            ORDER BY grade
            LIMIT ?
            """
            params = [f"%{request.query}%", request.limit]
            
        elif request.field == "department":
            query = """
            SELECT DISTINCT json_extract(r.result_data, '$.ë¶€ì„œ') as dept
            FROM results r
            WHERE json_extract(r.result_data, '$.ë¶€ì„œ') LIKE ?
            AND json_extract(r.result_data, '$.ë¶€ì„œ') IS NOT NULL
            ORDER BY dept
            LIMIT ?
            """
            params = [f"%{request.query}%", request.limit]
            
        else:
            return {"suggestions": [], "message": "ì§€ì›í•˜ì§€ ì•ŠëŠ” í•„ë“œì…ë‹ˆë‹¤"}
        
        # ğŸ”¥ ë‹¨ì¼ ì»¤ë„¥ì…˜ ì‚¬ìš©
        try:
            conn = await db_service.get_connection()
            cursor = await conn.execute(query, params)
            rows = await cursor.fetchall()
            await cursor.close()
            await conn.close()
            
            suggestions = [row[0] for row in rows if row[0]]
            
        except Exception as db_error:
            logger.error(f"âŒ ìë™ì™„ì„± DB ì˜¤ë¥˜: {db_error}")
            if 'conn' in locals():
                await conn.close()
            raise HTTPException(status_code=500, detail=f"ìë™ì™„ì„± DB ì˜¤ë¥˜: {str(db_error)}")
        
        logger.info(f"âœ… ìë™ì™„ì„±: {len(suggestions)}ê°œ ì œì•ˆ")
        return {
            "suggestions": suggestions,
            "field": request.field,
            "query": request.query,
            "total_found": len(suggestions)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ìë™ì™„ì„± ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ìë™ì™„ì„± ì‹¤íŒ¨: {str(e)}")

# ğŸ¯ íŠ¹ì • ì§ì› ë¶„ì„ íˆìŠ¤í† ë¦¬ - ìµœì í™”
@router.get("/employee/{uid}")
async def get_employee_history(
    uid: str,
    limit: int = Query(10, ge=1, le=100),
    include_details: bool = Query(False)
):
    """íŠ¹ì • ì§ì›ì˜ ë¶„ì„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ - ë‹¨ì¼ ì»¤ë„¥ì…˜ ì‚¬ìš©"""
    try:
        logger.info(f"ğŸ‘¤ ì§ì› íˆìŠ¤í† ë¦¬ ì¡°íšŒ: {uid}")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        query = """
        SELECT 
            r.result_data,
            j.created_at,
            j.id,
            j.file_id
        FROM results r
        JOIN jobs j ON r.job_id = j.id
        WHERE r.uid = ? AND j.status = 'completed'
        ORDER BY j.created_at DESC
        LIMIT ?
        """
        
        # ğŸ”¥ ë‹¨ì¼ ì»¤ë„¥ì…˜ ì‚¬ìš©
        rows = []
        try:
            conn = await db_service.get_connection()
            cursor = await conn.execute(query, [uid, limit])
            rows = await cursor.fetchall()
            await cursor.close()
            await conn.close()
            
        except Exception as db_error:
            logger.error(f"âŒ ì§ì› íˆìŠ¤í† ë¦¬ DB ì˜¤ë¥˜: {db_error}")
            if 'conn' in locals():
                await conn.close()
            raise HTTPException(status_code=500, detail=f"íˆìŠ¤í† ë¦¬ DB ì˜¤ë¥˜: {str(db_error)}")
        
        if not rows:
            raise HTTPException(status_code=404, detail=f"ì§ì› {uid}ì˜ ë¶„ì„ ê¸°ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        history = []
        scores = []
        grades = []
        dates = []
        
        for row in rows:
            try:
                import json
                result_data = json.loads(row[0]) if isinstance(row[0], str) else row[0]
                
                analysis_date = row[1]
                score = result_data.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0)
                grade = result_data.get("OKë“±ê¸‰", "")
                
                entry = {
                    "analysis_date": analysis_date,
                    "job_id": row[2],
                    "file_id": row[3],
                    "score": score,
                    "grade": grade,
                    "confidence": result_data.get("ë¶„ì„ì‹ ë¢°ë„", 0)
                }
                
                if include_details:
                    entry["full_data"] = result_data
                else:
                    entry["summary"] = {
                        "top_strength": result_data.get("ì£¼ìš”ê°•ì _1ì˜ì—­", ""),
                        "improvement_area": result_data.get("ê°œì„ í•„ìš”_1ì˜ì—­", ""),
                        "ai_suggestion": result_data.get("AIê°œì„ ì œì•ˆ_1", "")
                    }
                
                history.append(entry)
                scores.append(score)
                grades.append(grade)
                dates.append(analysis_date)
                
            except Exception as e:
                logger.error(f"âš ï¸ íˆìŠ¤í† ë¦¬ í•­ëª© ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        # í†µê³„ ê³„ì‚°
        trend_analysis = {}
        if scores:
            latest_score = scores[0]
            previous_score = scores[1] if len(scores) > 1 else scores[0]
            score_change = latest_score - previous_score
            
            trend_analysis = {
                "latest_score": latest_score,
                "previous_score": previous_score,
                "score_change": round(score_change, 1),
                "trend": "ìƒìŠ¹" if score_change > 0 else "í•˜ë½" if score_change < 0 else "ìœ ì§€",
                "highest_score": max(scores),
                "lowest_score": min(scores),
                "average_score": round(np.mean(scores), 1),
                "analysis_count": len(scores)
            }
        
        response = {
            "uid": uid,
            "history": history,
            "trend_analysis": trend_analysis,
            "grade_changes": _analyze_grade_changes(grades, dates),
            "total_analyses": len(history)
        }
        
        logger.info(f"âœ… ì§ì› íˆìŠ¤í† ë¦¬: {len(history)}ê°œ ë¶„ì„ ê¸°ë¡")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì§ì› íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

def _analyze_grade_changes(grades, dates):
    """ë“±ê¸‰ ë³€í™” ë¶„ì„"""
    if len(grades) < 2:
        return {"message": "ë“±ê¸‰ ë³€í™”ë¥¼ ë¶„ì„í•˜ê¸°ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
    
    latest_grade = grades[0]
    previous_grade = grades[1]
    
    # ë“±ê¸‰ ì ìˆ˜ ë§¤í•‘
    grade_scores = {
        "OKâ˜…â˜…â˜…": 100, "OKâ˜…â˜…": 90, "OKâ˜…": 85, "OK A": 80,
        "OK B+": 75, "OK B": 70, "OK C": 60, "OK D": 40
    }
    
    latest_score = grade_scores.get(latest_grade, 50)
    previous_score = grade_scores.get(previous_grade, 50)
    
    return {
        "latest_grade": latest_grade,
        "previous_grade": previous_grade,
        "grade_change": "ìƒìŠ¹" if latest_score > previous_score else "í•˜ë½" if latest_score < previous_score else "ìœ ì§€",
        "grade_history": grades[:5]  # ìµœê·¼ 5ê°œ
    }

# ğŸ¯ ë‹¤ì¤‘ ì§ì› ë¹„êµ API - ì»¤ë„¥ì…˜ ìµœì í™” (í•µì‹¬ ìˆ˜ì •)
@router.post("/compare")
async def compare_employees(request: CompareRequest):
    """
    ë‹¤ì¤‘ ì§ì› ì„±ê³¼ ë¹„êµ ë¶„ì„ - ë‹¨ì¼ ì»¤ë„¥ì…˜ìœ¼ë¡œ ìµœì í™”
    ğŸ”¥ ê°€ì¥ ì¤‘ìš”í•œ ìˆ˜ì •: ë£¨í”„ì—ì„œ ì»¤ë„¥ì…˜ ì¤‘ë³µ ìƒì„± ë¬¸ì œ í•´ê²°
    """
    try:
        logger.info(f"ğŸ”„ ì§ì› ë¹„êµ ìš”ì²­: {request.uids}")
        
        if len(request.uids) < 2:
            raise HTTPException(status_code=400, detail="ë¹„êµë¥¼ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ëª…ì˜ ì§ì›ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        if len(request.uids) > 10:
            raise HTTPException(status_code=400, detail="í•œ ë²ˆì— ìµœëŒ€ 10ëª…ê¹Œì§€ë§Œ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: ë‹¨ì¼ ì»¤ë„¥ì…˜ìœ¼ë¡œ ëª¨ë“  ì§ì› ë°ì´í„° ì¡°íšŒ
        comparison_data = []
        
        try:
            # ë‹¨ì¼ ì»¤ë„¥ì…˜ ìƒì„±
            conn = await db_service.get_connection()
            
            # ì „ì²´ ì§ì›ì˜ ìµœì‹  ë¶„ì„ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ì¡°íšŒ
            uid_placeholders = ", ".join(["?" for _ in request.uids])
            batch_query = f"""
            WITH latest_analysis AS (
                SELECT 
                    r.uid,
                    r.result_data,
                    j.created_at,
                    ROW_NUMBER() OVER (PARTITION BY r.uid ORDER BY j.created_at DESC) as rn
                FROM results r
                JOIN jobs j ON r.job_id = j.id
                WHERE r.uid IN ({uid_placeholders}) AND j.status = 'completed'
            )
            SELECT uid, result_data, created_at
            FROM latest_analysis
            WHERE rn = 1
            ORDER BY uid
            """
            
            cursor = await conn.execute(batch_query, request.uids)
            rows = await cursor.fetchall()
            await cursor.close()
            await conn.close()
            
            # ê²°ê³¼ ì²˜ë¦¬
            found_uids = set()
            for row in rows:
                try:
                    import json
                    uid = row[0]
                    result_data = json.loads(row[1]) if isinstance(row[1], str) else row[1]
                    analysis_date = row[2]
                    
                    employee_data = {
                        "uid": uid,
                        "analysis_date": analysis_date,
                        "overall_score": result_data.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0),
                        "grade": result_data.get("OKë“±ê¸‰", ""),
                        "dimension_scores": {
                            "ì—…ë¬´ì„±ê³¼": result_data.get("ì—…ë¬´ì„±ê³¼_ì ìˆ˜", 0),
                            "KPIë‹¬ì„±": result_data.get("KPIë‹¬ì„±_ì ìˆ˜", 0),
                            "íƒœë„ë§ˆì¸ë“œ": result_data.get("íƒœë„ë§ˆì¸ë“œ_ì ìˆ˜", 0),
                            "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜": result_data.get("ì»¤ë®¤ë‹ˆì¼€ì´ì…˜_ì ìˆ˜", 0),
                            "ë¦¬ë”ì‹­í˜‘ì—…": result_data.get("ë¦¬ë”ì‹­í˜‘ì—…_ì ìˆ˜", 0),
                            "ì „ë¬¸ì„±í•™ìŠµ": result_data.get("ì „ë¬¸ì„±í•™ìŠµ_ì ìˆ˜", 0),
                            "ì°½ì˜í˜ì‹ ": result_data.get("ì°½ì˜í˜ì‹ _ì ìˆ˜", 0),
                            "ì¡°ì§ì ì‘": result_data.get("ì¡°ì§ì ì‘_ì ìˆ˜", 0)
                        },
                        "strengths": [
                            result_data.get("ì£¼ìš”ê°•ì _1ì˜ì—­", ""),
                            result_data.get("ì£¼ìš”ê°•ì _2ì˜ì—­", ""),
                            result_data.get("ì£¼ìš”ê°•ì _3ì˜ì—­", "")
                        ],
                        "improvements": [
                            result_data.get("ê°œì„ í•„ìš”_1ì˜ì—­", ""),
                            result_data.get("ê°œì„ í•„ìš”_2ì˜ì—­", ""),
                            result_data.get("ê°œì„ í•„ìš”_3ì˜ì—­", "")
                        ]
                    }
                    
                    comparison_data.append(employee_data)
                    found_uids.add(uid)
                    
                except Exception as e:
                    logger.error(f"âš ï¸ ì§ì› {row[0]} ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # ì°¾ì§€ ëª»í•œ ì§ì›ë“¤ ë¡œê¹…
            missing_uids = set(request.uids) - found_uids
            if missing_uids:
                logger.warning(f"âš ï¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ì§ì›ë“¤: {missing_uids}")
                
        except Exception as db_error:
            logger.error(f"âŒ ë¹„êµ ë¶„ì„ DB ì˜¤ë¥˜: {db_error}")
            if 'conn' in locals():
                await conn.close()
            raise HTTPException(status_code=500, detail=f"ë¹„êµ ë¶„ì„ DB ì˜¤ë¥˜: {str(db_error)}")
        
        if len(comparison_data) < 2:
            raise HTTPException(status_code=404, detail="ë¹„êµí•  ìˆ˜ ìˆëŠ” ìœ íš¨í•œ ë¶„ì„ ê²°ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤")
        
        # ë¹„êµ ë¶„ì„ ìˆ˜í–‰
        comparison_analysis = _perform_comparison_analysis(comparison_data, request.dimensions)
        
        response = {
            "employees": comparison_data,
            "comparison_analysis": comparison_analysis,
            "metadata": {
                "compared_count": len(comparison_data),
                "requested_uids": request.uids,
                "found_uids": list(found_uids),
                "missing_uids": list(set(request.uids) - found_uids),
                "comparison_date": datetime.now().isoformat(),
                "dimensions_analyzed": request.dimensions or ["ì „ì²´"]
            }
        }
        
        logger.info(f"âœ… ì§ì› ë¹„êµ ì™„ë£Œ: {len(comparison_data)}ëª… (ìš”ì²­: {len(request.uids)}ëª…)")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì§ì› ë¹„êµ ì˜¤ë¥˜: {e}")
        logger.error(f"ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

def _perform_comparison_analysis(employees_data, dimensions=None):
    """ë¹„êµ ë¶„ì„ ìˆ˜í–‰"""
    if not employees_data:
        return {}
    
    # ì¢…í•© ì ìˆ˜ ë¹„êµ
    scores = [emp["overall_score"] for emp in employees_data]
    highest_performer = max(employees_data, key=lambda x: x["overall_score"])
    lowest_performer = min(employees_data, key=lambda x: x["overall_score"])
    
    # ì°¨ì›ë³„ ë¹„êµ (ìš”ì²­ëœ ì°¨ì›ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì°¨ì›ë§Œ, ì—†ìœ¼ë©´ ì „ì²´)
    dimension_comparison = {}
    all_dimensions = ["ì—…ë¬´ì„±ê³¼", "KPIë‹¬ì„±", "íƒœë„ë§ˆì¸ë“œ", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", 
                     "ë¦¬ë”ì‹­í˜‘ì—…", "ì „ë¬¸ì„±í•™ìŠµ", "ì°½ì˜í˜ì‹ ", "ì¡°ì§ì ì‘"]
    
    target_dimensions = dimensions if dimensions else all_dimensions
    
    for dimension in target_dimensions:
        if dimension in all_dimensions:
            dim_scores = [emp["dimension_scores"].get(dimension, 0) for emp in employees_data]
            dimension_comparison[dimension] = {
                "scores": dict(zip([emp["uid"] for emp in employees_data], dim_scores)),
                "highest": max(dim_scores),
                "lowest": min(dim_scores),
                "average": round(np.mean(dim_scores), 1),
                "range": max(dim_scores) - min(dim_scores)
            }
    
    return {
        "overall_comparison": {
            "highest_performer": {
                "uid": highest_performer["uid"],
                "score": highest_performer["overall_score"],
                "grade": highest_performer["grade"]
            },
            "lowest_performer": {
                "uid": lowest_performer["uid"],
                "score": lowest_performer["overall_score"],
                "grade": lowest_performer["grade"]
            },
            "score_range": max(scores) - min(scores),
            "average_score": round(np.mean(scores), 1)
        },
        "dimension_comparison": dimension_comparison,
        "insights": _generate_comparison_insights(employees_data)
    }

def _generate_comparison_insights(employees_data):
    """ë¹„êµ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    insights = []
    
    if len(employees_data) >= 2:
        scores = [emp["overall_score"] for emp in employees_data]
        score_range = max(scores) - min(scores)
        
        if score_range > 20:
            insights.append("ì§ì› ê°„ ì„±ê³¼ í¸ì°¨ê°€ í½ë‹ˆë‹¤. ì €ì„±ê³¼ìì— ëŒ€í•œ ì§‘ì¤‘ ì§€ì›ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        elif score_range < 5:
            insights.append("ì§ì› ê°„ ì„±ê³¼ê°€ ê· ë“±í•©ë‹ˆë‹¤. íŒ€ ì „ì²´ì˜ ì•ˆì •ì ì¸ ì„±ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        
        # ê°•ì  ë¶„ì„
        all_strengths = []
        for emp in employees_data:
            all_strengths.extend([s for s in emp["strengths"] if s])
        
        common_strengths = Counter(all_strengths).most_common(3)
        
        if common_strengths:
            insights.append(f"ê³µí†µ ê°•ì  ì˜ì—­: {', '.join([s[0] for s in common_strengths])}")
    
    return insights

# ğŸ¯ íŒ€ë³„ ë¶„ì„ í˜„í™© - ìµœì í™”
@router.get("/team-summary")
async def get_team_summary(
    department: Optional[str] = Query(None),
    date_from: Optional[str] = Query(None),
    date_to: Optional[str] = Query(None)
):
    """íŒ€/ë¶€ì„œë³„ ë¶„ì„ í˜„í™© ìš”ì•½ - ë‹¨ì¼ ì»¤ë„¥ì…˜ ì‚¬ìš©"""
    try:
        logger.info(f"ğŸ¢ íŒ€ ìš”ì•½ ì¡°íšŒ: ë¶€ì„œ={department}")
        
        db_service = get_db_service()
        await db_service.init_database()
        
        # ê¸°ë³¸ ì¿¼ë¦¬
        base_query = """
        SELECT 
            r.result_data,
            j.created_at
        FROM results r
        JOIN jobs j ON r.job_id = j.id
        WHERE j.status = 'completed'
        """
        
        params = []
        
        # ë¶€ì„œ í•„í„°
        if department:
            base_query += " AND r.result_data LIKE ?"
            params.append(f'%"ë¶€ì„œ":"%{department}%"%')
        
        # ë‚ ì§œ í•„í„°
        if date_from:
            base_query += " AND j.created_at >= ?"
            params.append(date_from)
        
        if date_to:
            base_query += " AND j.created_at <= ?"
            params.append(date_to)
        
        # ğŸ”¥ ë‹¨ì¼ ì»¤ë„¥ì…˜ ì‚¬ìš©
        rows = []
        try:
            conn = await db_service.get_connection()
            cursor = await conn.execute(base_query, params)
            rows = await cursor.fetchall()
            await cursor.close()
            await conn.close()
            
        except Exception as db_error:
            logger.error(f"âŒ íŒ€ ìš”ì•½ DB ì˜¤ë¥˜: {db_error}")
            if 'conn' in locals():
                await conn.close()
            raise HTTPException(status_code=500, detail=f"íŒ€ ìš”ì•½ DB ì˜¤ë¥˜: {str(db_error)}")
        
        # ë°ì´í„° ì²˜ë¦¬
        team_data = {}
        total_analyses = 0
        
        for row in rows:
            try:
                import json
                result_data = json.loads(row[0]) if isinstance(row[0], str) else row[0]
                
                dept = result_data.get("ë¶€ì„œ", "ë¯¸ë¶„ë¥˜")
                score = result_data.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0)
                grade = result_data.get("OKë“±ê¸‰", "")
                
                if dept not in team_data:
                    team_data[dept] = {
                        "department": dept,
                        "scores": [],
                        "grades": [],
                        "analysis_count": 0
                    }
                
                team_data[dept]["scores"].append(score)
                team_data[dept]["grades"].append(grade)
                team_data[dept]["analysis_count"] += 1
                total_analyses += 1
                
            except Exception as e:
                logger.error(f"âš ï¸ íŒ€ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        # íŒ€ë³„ í†µê³„ ê³„ì‚°
        team_summary = []
        for dept, data in team_data.items():
            if data["scores"]:
                summary = {
                    "department": dept,
                    "analysis_count": data["analysis_count"],
                    "average_score": round(np.mean(data["scores"]), 1),
                    "highest_score": max(data["scores"]),
                    "lowest_score": min(data["scores"]),
                    "grade_distribution": dict(Counter(data["grades"])),
                    "performance_level": _classify_team_performance(np.mean(data["scores"]))
                }
                team_summary.append(summary)
        
        # ì •ë ¬ (í‰ê·  ì ìˆ˜ ê¸°ì¤€)
        team_summary.sort(key=lambda x: x["average_score"], reverse=True)
        
        response = {
            "team_summary": team_summary,
            "overall_statistics": {
                "total_departments": len(team_summary),
                "total_analyses": total_analyses,
                "overall_average": round(np.mean([t["average_score"] for t in team_summary]), 1) if team_summary else 0,
                "best_performing_team": team_summary[0]["department"] if team_summary else None,
                "analysis_period": {
                    "from": date_from,
                    "to": date_to
                }
            }
        }
        
        logger.info(f"âœ… íŒ€ ìš”ì•½ ì™„ë£Œ: {len(team_summary)}ê°œ ë¶€ì„œ")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ íŒ€ ìš”ì•½ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"íŒ€ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

def _classify_team_performance(avg_score):
    """íŒ€ ì„±ê³¼ ìˆ˜ì¤€ ë¶„ë¥˜"""
    if avg_score >= 90:
        return "ìµœìš°ìˆ˜"
    elif avg_score >= 80:
        return "ìš°ìˆ˜"
    elif avg_score >= 70:
        return "ì–‘í˜¸"
    elif avg_score >= 60:
        return "ë³´í†µ"
    else:
        return "ê°œì„ í•„ìš”"

# ğŸ¯ ì¦ê²¨ì°¾ê¸° ê´€ë¦¬ - ë©”ëª¨ë¦¬ ê¸°ë°˜ (ì‹¤ì œë¡œëŠ” DB ì €ì¥)
favorites_storage = {}  # user_id -> [uid1, uid2, ...]

@router.post("/favorites/add")
async def add_favorite(request: FavoriteRequest):
    """ì¦ê²¨ì°¾ê¸° ì¶”ê°€"""
    try:
        logger.info(f"â­ ì¦ê²¨ì°¾ê¸° ì¶”ê°€: {request.uid}")
        
        user_favorites = favorites_storage.get(request.user_id, [])
        
        # ì¤‘ë³µ ì²´í¬
        existing = next((f for f in user_favorites if f["uid"] == request.uid), None)
        if existing:
            return {"status": "already_exists", "message": "ì´ë¯¸ ì¦ê²¨ì°¾ê¸°ì— ìˆìŠµë‹ˆë‹¤"}
        
        # ì¦ê²¨ì°¾ê¸° ì¶”ê°€
        favorite_entry = {
            "uid": request.uid,
            "note": request.note,
            "added_at": datetime.now().isoformat(),
            "id": len(user_favorites) + 1
        }
        
        user_favorites.append(favorite_entry)
        favorites_storage[request.user_id] = user_favorites
        
        logger.info(f"âœ… ì¦ê²¨ì°¾ê¸° ì¶”ê°€ ì™„ë£Œ: {request.uid}")
        return {
            "status": "added",
            "favorite": favorite_entry,
            "total_favorites": len(user_favorites)
        }
        
    except Exception as e:
        logger.error(f"âŒ ì¦ê²¨ì°¾ê¸° ì¶”ê°€ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¦ê²¨ì°¾ê¸° ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")

@router.delete("/favorites/remove/{uid}")
async def remove_favorite(uid: str, user_id: str = Query("default_user")):
    """ì¦ê²¨ì°¾ê¸° ì œê±°"""
    try:
        logger.info(f"ğŸ—‘ï¸ ì¦ê²¨ì°¾ê¸° ì œê±°: {uid}")
        
        user_favorites = favorites_storage.get(user_id, [])
        original_count = len(user_favorites)
        user_favorites = [f for f in user_favorites if f["uid"] != uid]
        
        if len(user_favorites) == original_count:
            raise HTTPException(status_code=404, detail="ì¦ê²¨ì°¾ê¸°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        favorites_storage[user_id] = user_favorites
        
        logger.info(f"âœ… ì¦ê²¨ì°¾ê¸° ì œê±° ì™„ë£Œ: {uid}")
        return {
            "status": "removed",
            "uid": uid,
            "remaining_count": len(user_favorites)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ì¦ê²¨ì°¾ê¸° ì œê±° ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¦ê²¨ì°¾ê¸° ì œê±° ì‹¤íŒ¨: {str(e)}")

@router.get("/favorites")
async def get_favorites(
    user_id: str = Query("default_user"),
    include_details: bool = Query(False)
):
    """ì¦ê²¨ì°¾ê¸° ëª©ë¡ ì¡°íšŒ - ìƒì„¸ ì •ë³´ í¬í•¨ ì‹œ ë‹¨ì¼ ì»¤ë„¥ì…˜ ì‚¬ìš©"""
    try:
        logger.info(f"â­ ì¦ê²¨ì°¾ê¸° ëª©ë¡ ì¡°íšŒ: {user_id}")
        
        user_favorites = favorites_storage.get(user_id, [])
        
        if not user_favorites:
            return {
                "favorites": [],
                "total_count": 0,
                "message": "ì¦ê²¨ì°¾ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤"
            }
        
        # ìƒì„¸ ì •ë³´ í¬í•¨ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
        if include_details:
            # ğŸ”¥ ë‹¨ì¼ ì»¤ë„¥ì…˜ìœ¼ë¡œ ëª¨ë“  ì¦ê²¨ì°¾ê¸° ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
            db_service = get_db_service()
            await db_service.init_database()
            
            detailed_favorites = []
            favorite_uids = [f["uid"] for f in user_favorites]
            
            try:
                conn = await db_service.get_connection()
                
                # ë°°ì¹˜ë¡œ ëª¨ë“  ì¦ê²¨ì°¾ê¸°ì˜ ìµœì‹  ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
                uid_placeholders = ", ".join(["?" for _ in favorite_uids])
                batch_query = f"""
                WITH latest_analysis AS (
                    SELECT 
                        r.uid,
                        r.result_data,
                        j.created_at,
                        ROW_NUMBER() OVER (PARTITION BY r.uid ORDER BY j.created_at DESC) as rn
                    FROM results r
                    JOIN jobs j ON r.job_id = j.id
                    WHERE r.uid IN ({uid_placeholders}) AND j.status = 'completed'
                )
                SELECT uid, result_data, created_at
                FROM latest_analysis
                WHERE rn = 1
                """
                
                cursor = await conn.execute(batch_query, favorite_uids)
                analysis_results = await cursor.fetchall()
                await cursor.close()
                await conn.close()
                
                # ë¶„ì„ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                analysis_dict = {}
                for row in analysis_results:
                    try:
                        import json
                        uid = row[0]
                        result_data = json.loads(row[1]) if isinstance(row[1], str) else row[1]
                        analysis_dict[uid] = {
                            "latest_score": result_data.get("AIRISS_v4_ì¢…í•©ì ìˆ˜", 0),
                            "latest_grade": result_data.get("OKë“±ê¸‰", ""),
                            "last_analysis": row[2],
                            "has_analysis": True
                        }
                    except Exception as e:
                        logger.error(f"âš ï¸ ì¦ê²¨ì°¾ê¸° ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜ ({row[0]}): {e}")
                        analysis_dict[row[0]] = {"has_analysis": False, "error": str(e)}
                
                # ì¦ê²¨ì°¾ê¸°ì™€ ë¶„ì„ ê²°ê³¼ ê²°í•©
                for favorite in user_favorites:
                    uid = favorite["uid"]
                    detailed_favorite = favorite.copy()
                    
                    if uid in analysis_dict:
                        detailed_favorite.update(analysis_dict[uid])
                    else:
                        detailed_favorite.update({
                            "has_analysis": False,
                            "message": "ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
                        })
                    
                    detailed_favorites.append(detailed_favorite)
                    
            except Exception as db_error:
                logger.error(f"âŒ ì¦ê²¨ì°¾ê¸° ìƒì„¸ ì¡°íšŒ DB ì˜¤ë¥˜: {db_error}")
                if 'conn' in locals():
                    await conn.close()
                # DB ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ì •ë³´ë§Œ ë°˜í™˜
                detailed_favorites = [
                    {**favorite, "has_analysis": False, "error": str(db_error)} 
                    for favorite in user_favorites
                ]
            
            return {
                "favorites": detailed_favorites,
                "total_count": len(detailed_favorites),
                "include_details": True
            }
        else:
            return {
                "favorites": user_favorites,
                "total_count": len(user_favorites),
                "include_details": False
            }
        
    except Exception as e:
        logger.error(f"âŒ ì¦ê²¨ì°¾ê¸° ì¡°íšŒ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"ì¦ê²¨ì°¾ê¸° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@router.get("/favorites/check/{uid}")
async def check_favorite_status(uid: str, user_id: str = Query("default_user")):
    """íŠ¹ì • ì§ì›ì˜ ì¦ê²¨ì°¾ê¸° ìƒíƒœ í™•ì¸"""
    try:
        user_favorites = favorites_storage.get(user_id, [])
        is_favorite = any(f["uid"] == uid for f in user_favorites)
        
        return {
            "uid": uid,
            "is_favorite": is_favorite,
            "total_favorites": len(user_favorites)
        }
        
    except Exception as e:
        logger.error(f"âŒ ì¦ê²¨ì°¾ê¸° ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
        return {"uid": uid, "is_favorite": False, "error": str(e)}

# ğŸ¯ ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
search_history = []

@router.get("/recent-searches")
async def get_recent_searches(limit: int = Query(10, ge=1, le=50)):
    """ìµœê·¼ ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
    try:
        recent = search_history[-limit:]
        return {
            "recent_searches": recent,
            "total_count": len(search_history)
        }
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return {"recent_searches": [], "total_count": 0}

@router.post("/save-search")
async def save_search_history(search_term: str):
    """ê²€ìƒ‰ì–´ íˆìŠ¤í† ë¦¬ ì €ì¥"""
    try:
        if search_term and search_term.strip():
            search_entry = {
                "query": search_term.strip(),
                "timestamp": datetime.now().isoformat(),
                "id": len(search_history) + 1
            }
            search_history.append(search_entry)
            
            # ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ ìœ ì§€
            if len(search_history) > 100:
                search_history.pop(0)
            
            return {"status": "saved", "entry": search_entry}
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ íˆìŠ¤í† ë¦¬ ì €ì¥ ì˜¤ë¥˜: {e}")
        return {"status": "error", "message": str(e)}

# ğŸ¯ í—¬ìŠ¤ì²´í¬
@router.get("/health")
async def search_health_check():
    """ê²€ìƒ‰ API í—¬ìŠ¤ì²´í¬"""
    try:
        db_service = get_db_service()
        db_status = "connected" if db_service else "disconnected"
        
        return {
            "status": "healthy",
            "service": "AIRISS Search API v4.1 Enhanced - Connection Optimized",
            "database": db_status,
            "features": [
                "ê³ ê¸‰ ê²€ìƒ‰", "ìë™ì™„ì„±", "ì§ì› íˆìŠ¤í† ë¦¬", 
                "ë‹¤ì¤‘ ë¹„êµ", "íŒ€ ë¶„ì„", "ê²€ìƒ‰ íˆìŠ¤í† ë¦¬", "ì¦ê²¨ì°¾ê¸°"
            ],
            "optimizations": [
                "ë‹¨ì¼ ì»¤ë„¥ì…˜ ì‚¬ìš©", "ë°°ì¹˜ ì¿¼ë¦¬", "ì»¤ë„¥ì…˜ í’€ë§", "ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”"
            ],
            "favorites_count": sum(len(favs) for favs in favorites_storage.values()),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
